{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week4.ipynb","provenance":[],"authorship_tag":"ABX9TyNOjdOXPqAirDO6VdXduddk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"rycIeM6Yrwt8","executionInfo":{"status":"ok","timestamp":1633006099961,"user_tz":-540,"elapsed":289,"user":{"displayName":"조원기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03176743448410123113"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","from torch import Tensor\n","\n","class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model : int, dropout : float = 0.1, max_len : int = 5000) -> None:\n","    super().__init__()\n","\n","    self.dropout = nn.Dropout(p = dropout)\n","    pe = self.make_pe(d_model = d_model, max_len = max_len)\n","    self.register_buffer(\"pe\", pe)\n","\n","  @staticmethod\n","  def make_pe(d_model : int, max_len : int) -> torch.Tensor:\n","    pe = torch.zeros(max_len, d_model)\n","    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","    pe[:, 0::2] = torch.sin(position * div_term)\n","    pe[:, 1::2] = torch.cos(position * div_term)\n","    pe = pe.unsqueeze(1)\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    # x.shape = (S, B, d_model)\n","    assert x.shape[2] == self.pe.shape[2]  # type: ignore\n","    x = x + self.pe[: x.size(0)]  # type: ignore\n","    return self.dropout(x)\n","\n","def generate_square_subsequent_mask(size : int) -> torch.Tensor:\n","  mask = (torch.triu(torch.ones(size, size))==1).transpose(0, 1) #역삼각행렬을 전치\n","  mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n","  return mask"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptkdTIAAvFpS","executionInfo":{"status":"ok","timestamp":1633016512493,"user_tz":-540,"elapsed":308,"user":{"displayName":"조원기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03176743448410123113"}}},"source":["from typing import Any, Dict, Union, Tuple\n","import argparse\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# Common type hints\n","Param2D = Union[int, Tuple[int, int]]\n","\n","CONV_DIM = 32\n","FC_DIM = 512\n","WINDOW_WIDTH = 16\n","WINDOW_STRIDE = 8\n","\n","\n","class ConvBlock(nn.Module):\n","    \"\"\"\n","    Simple 3x3 conv with padding size 1 (to leave the input size unchanged), followed by a ReLU.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        input_channels: int,\n","        output_channels: int,\n","        kernel_size: Param2D = 3,\n","        stride: Param2D = 1,\n","        padding: Param2D = 1,\n","    ) -> None:\n","        super().__init__()\n","        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Parameters\n","        ----------\n","        x\n","            of dimensions (B, C, H, W)\n","        Returns\n","        -------\n","        torch.Tensor\n","            of dimensions (B, C, H, W)\n","        \"\"\"\n","        c = self.conv(x)\n","        r = self.relu(c)\n","        return r\n","\n","\n","class LineCNN(nn.Module):\n","    \"\"\"\n","    Model that uses a simple CNN to process an image of a line of characters with a window, outputs a sequence of logits\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        data_config: Dict[str, Any],\n","        args: argparse.Namespace = None,\n","    ) -> None:\n","        super().__init__()\n","        self.data_config = data_config\n","        self.args = vars(args) if args is not None else {}\n","        self.num_classes = len(data_config[\"mapping\"])\n","        self.output_length = data_config[\"output_dims\"][0]\n","\n","        _C, H, _W = data_config[\"input_dims\"]\n","        conv_dim = self.args.get(\"conv_dim\", CONV_DIM)\n","        fc_dim = self.args.get(\"fc_dim\", FC_DIM)\n","        self.WW = self.args.get(\"window_width\", WINDOW_WIDTH)\n","        self.WS = self.args.get(\"window_stride\", WINDOW_STRIDE)\n","        self.limit_output_length = self.args.get(\"limit_output_length\", False)\n","\n","        # Input is (1, H, W)\n","        self.convs = nn.Sequential(\n","            ConvBlock(1, conv_dim),\n","            ConvBlock(conv_dim, conv_dim),\n","            ConvBlock(conv_dim, conv_dim, stride=2),\n","            ConvBlock(conv_dim, conv_dim),\n","            ConvBlock(conv_dim, conv_dim * 2, stride=2),\n","            ConvBlock(conv_dim * 2, conv_dim * 2),\n","            ConvBlock(conv_dim * 2, conv_dim * 4, stride=2),\n","            ConvBlock(conv_dim * 4, conv_dim * 4),\n","            ConvBlock(\n","                conv_dim * 4, fc_dim, kernel_size=(H // 8, self.WW // 8), stride=(H // 8, self.WS // 8), padding=0\n","            ),\n","        )\n","        self.fc1 = nn.Linear(fc_dim, fc_dim)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc2 = nn.Linear(fc_dim, self.num_classes)\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        \"\"\"\n","        Initialize weights in a better way than default.\n","        See https://github.com/pytorch/pytorch/issues/18182\n","        \"\"\"\n","        for m in self.modules():\n","            if type(m) in {\n","                nn.Conv2d,\n","                nn.Conv3d,\n","                nn.ConvTranspose2d,\n","                nn.ConvTranspose3d,\n","                nn.Linear,\n","            }:\n","                nn.init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_out\", nonlinearity=\"relu\")\n","                if m.bias is not None:\n","                    _fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(  # pylint: disable=protected-access\n","                        m.weight.data\n","                    )\n","                    bound = 1 / math.sqrt(fan_out)\n","                    nn.init.normal_(m.bias, -bound, bound)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Parameters\n","        ----------\n","        x\n","            (B, 1, H, W) input image\n","        Returns\n","        -------\n","        torch.Tensor\n","            (B, C, S) logits, where S is the length of the sequence and C is the number of classes\n","            S can be computed from W and self.window_width\n","            C is self.num_classes\n","        \"\"\"\n","        _B, _C, _H, _W = x.shape\n","        x = self.convs(x)  # (B, FC_DIM, 1, Sx)\n","        x = x.squeeze(2).permute(0, 2, 1)  # (B, S, FC_DIM)\n","        x = F.relu(self.fc1(x))  # -> (B, S, FC_DIM)\n","        x = self.dropout(x)\n","        x = self.fc2(x)  # (B, S, C)\n","        x = x.permute(0, 2, 1)  # -> (B, C, S)\n","        if self.limit_output_length:\n","            x = x[:, :, : self.output_length]\n","        return x\n","\n","    @staticmethod\n","    def add_to_argparse(parser):\n","        parser.add_argument(\"--conv_dim\", type=int, default=CONV_DIM)\n","        parser.add_argument(\"--fc_dim\", type=int, default=FC_DIM)\n","        parser.add_argument(\n","            \"--window_width\",\n","            type=int,\n","            default=WINDOW_WIDTH,\n","            help=\"Width of the window that will slide over the input image.\",\n","        )\n","        parser.add_argument(\n","            \"--window_stride\",\n","            type=int,\n","            default=WINDOW_STRIDE,\n","            help=\"Stride of the window that will slide over the input image.\",\n","        )\n","        parser.add_argument(\"--limit_output_length\", action=\"store_true\", default=False)\n","        return parser"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6omcv-pvMZL","executionInfo":{"status":"ok","timestamp":1633016544274,"user_tz":-540,"elapsed":298,"user":{"displayName":"조원기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03176743448410123113"}}},"source":["TF_DIM = 256\n","TF_FC_DIM = 256\n","TF_DROPOUT = 0.4\n","TF_LAYERS = 4\n","TF_NHEAD = 4\n","\n","class LineCNNTransformer(nn.Module):\n","  def __init__(self, data_config:Dict[str, Any], args : argparse.Namespace = None,) ->None:\n","    super().__init__()\n","    self.data_config = data_config\n","    self.input_dims = data_config[\"input_dims\"]\n","    self.num_classes = len(data_config[\"mapping\"])\n","    inverse_mapping = {val: ind for ind, val in enumerate(data_config[\"mapping\"])}\n","    self.start_token = inverse_mapping[\"<S>\"]\n","    self.end_token = inverse_mapping[\"<E>\"]\n","    self.padding_token = inverse_mapping[\"<P>\"]\n","    self.max_output_length = data_config[\"output_dims\"][0]\n","    self.args = vars(args) if args is not None else {}\n","\n","\n","    self.dim = self.args.get(\"tf_dim\", TF_DIM)\n","    tf_fc_dim = self.args.get(\"tf_fc_dim\", TF_FC_DIM)\n","    tf_nhead = self.args.get(\"tf_nhead\", TF_NHEAD)\n","    tf_dropout = self.args.get(\"tf_dropout\", TF_DROPOUT)\n","    tf_layers = self.args.get(\"tf_layers\", TF_LAYERS)\n","\n","    data_config_for_line_cnn = {**data_config}\n","    data_config_for_line_cnn[\"mapping\"] = list(range(self.dim))\n","    self.line_cnn = LineCNN(data_config=data_config_for_line_cnn, args=args)\n","\n","    # LineCNN outputs (B, E, S) log probs, with E == dim\n","\n","    self.embedding = nn.Embedding(self.num_classes, self.dim)\n","    self.fc = nn.Linear(self.dim, self.num_classes)\n","\n","    self.pos_encoder = PositionalEncoding(d_model=self.dim)\n","\n","    self.y_mask = generate_square_subsequent_mask(self.max_output_length)\n","\n","    self.transformer_decoder = nn.TransformerDecoder(\n","        nn.TransformerDecoderLayer(d_model=self.dim, nhead=tf_nhead, dim_feedforward=tf_fc_dim, dropout=tf_dropout),\n","        num_layers=tf_layers,\n","    )\n","\n","    self.init_weights()  # This is empirically important\n","\n","  def init_weights(self):\n","      initrange = 0.1\n","      self.embedding.weight.data.uniform_(-initrange, initrange)\n","      self.fc.bias.data.zero_()\n","      self.fc.weight.data.uniform_(-initrange, initrange)\n","\n","  def encode(self, x : torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    x : (B, W, H)\n","    returns : (Sx, B, E)\n","    \"\"\"\n","    x = self.line_cnn(x) # image 입력\n","    x = x * math.sqrt(self.dim) \n","    x = x.permute(2,0,1)\n","    x = self.pos_encoder(x)\n","    return x\n","\n","  def decode(self, x, y):\n","    \"\"\"\n","    x (B, H, W)\n","    y (B, Sy)\n","    returns : (Sy, B, C) # C : num classes\n","    \"\"\"\n","    y_padding_mask = y == self.padding_token\n","    y = y.permute(1, 0)\n","    y = self.embedding(y) * math.sqrt(self.dim)\n","    y = self.pos_encoder(y)\n","    Sy = y.shape[0]\n","    y_mask = self.y_mask[:Sy, :Sy].type_as(x)\n","    output = self.transformer_decoder(\n","            tgt=y, memory=x, tgt_mask=y_mask, tgt_key_padding_mask=y_padding_mask\n","        )  # (Sy, B, E)\n","    output = self.fc(output)  # (Sy, B, C)\n","    return output\n","\n","  def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Parameters\n","    ----------\n","    x\n","        (B, H, W) image\n","    y\n","        (B, Sy) with elements in [0, C-1] where C is num_classes\n","    Returns\n","    -------\n","    torch.Tensor\n","        (B, C, Sy) logits\n","    \"\"\"\n","    x = self.encode(x)  # (Sx, B, E)\n","    output = self.decode(x, y)  # (Sy, B, C)\n","    return output.permute(1, 2, 0)  # (B, C, Sy)\n","\n","  def predict(self, x: torch.Tensor) -> torch.Tensor:\n","      \"\"\"\n","      Parameters\n","      ----------\n","      x\n","          (B, H, W) image\n","      Returns\n","      -------\n","      torch.Tensor\n","          (B, Sy) with elements in [0, C-1] where C is num_classes\n","      \"\"\"\n","      B = x.shape[0]\n","      S = self.max_output_length #문장 길이\n","      x = self.encode(x)  # (Sx, B, E)\n","\n","      output_tokens = (torch.ones((B, S)) * self.padding_token).type_as(x).long()  # (B, S)\n","      output_tokens[:, 0] = self.start_token  # Set start token\n","      for Sy in range(1, S):\n","        y = output_tokens[:, :Sy]  # (B, Sy)\n","        output = self.decode(x, y)  # (Sy, B, C)\n","        output = torch.argmax(output, dim=-1)  # (Sy, B)\n","        output_tokens[:, Sy] = output[-1:]  # Set the last output token\n","\n","      # Set all tokens after end token to be padding\n","      for Sy in range(1, S):\n","        ind = (output_tokens[:, Sy - 1] == self.end_token) | (output_tokens[:, Sy - 1] == self.padding_token)\n","        output_tokens[ind, Sy] = self.padding_token\n","\n","      return output_tokens  # (B, Sy)\n","\n","  @staticmethod\n","  def add_to_argparse(parser):\n","    LineCNN.add_to_argparse(parser)\n","    parser.add_argument(\"--tf_dim\", type=int, default=TF_DIM)\n","    parser.add_argument(\"--tf_fc_dim\", type=int, default=TF_FC_DIM)\n","    parser.add_argument(\"--tf_dropout\", type=float, default=TF_DROPOUT)\n","    parser.add_argument(\"--tf_layers\", type=int, default=TF_LAYERS)\n","    parser.add_argument(\"--tf_nhead\", type=int, default=TF_NHEAD)\n","    return parser\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtsw0dg0vNRT"},"source":[""],"execution_count":null,"outputs":[]}]}